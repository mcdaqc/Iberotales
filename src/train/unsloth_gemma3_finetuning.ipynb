{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2485ba0c-bb2a-487f-b6d7-2273c9ff0535",
   "metadata": {},
   "source": [
    "# Instalacion de dependenciasv (ONLY COLLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5496792-18ae-45a8-82e8-11e919569557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm==0.8.5.post1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518f40a-c151-420c-83f3-5c088d28106a",
   "metadata": {},
   "source": [
    "# Instalacion de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81e3a6-27fc-487f-b0e9-361c0cda29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "!pip install --upgrade pip\n",
    "\n",
    "!pip uninstall unsloth torch torchvision torchaudio xformers -y\n",
    "!pip cache purge\n",
    "\n",
    "!pip install torch==2.4.* torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "!pip install \"unsloth[cu124-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adace2-820d-489f-bcdf-60d2494796ad",
   "metadata": {},
   "source": [
    "# Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d553cf-917d-4180-86d6-25ec6a3cb9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.51.3 in /home/user/miniconda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: trl==0.15.2 in /home/user/miniconda/lib/python3.12/site-packages (0.15.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-deps --upgrade transformers==4.51.3 trl==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f48ae2e-fbba-406c-a198-bd58f2641fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: unsloth-zoo\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show unsloth-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "284d69ac-91d9-4822-89e0-34dd57a0274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping unsloth as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping unsloth-zoo as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall unsloth==2025.5.8 unsloth-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60a66f5-ed7e-4826-abd7-24eef5d61769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth==2025.5.7\n",
      "  Using cached unsloth-2025.5.7-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting unsloth-zoo==2025.5.8\n",
      "  Downloading unsloth_zoo-2025.5.8-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (2.7.0)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (0.0.30)\n",
      "Collecting bitsandbytes (from unsloth==2025.5.7)\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (24.2)\n",
      "Collecting tyro (from unsloth==2025.5.7)\n",
      "  Downloading tyro-0.9.22-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers!=4.47.0,==4.51.3 in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (4.51.3)\n",
      "Collecting datasets>=3.4.1 (from unsloth==2025.5.7)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (0.45.1)\n",
      "Requirement already satisfied: numpy in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (2.2.6)\n",
      "Collecting accelerate>=0.34.1 (from unsloth==2025.5.7)\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (0.15.2)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth==2025.5.7)\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<4.0.0 (from unsloth==2025.5.7)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: huggingface_hub in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (0.32.3)\n",
      "Collecting hf_transfer (from unsloth==2025.5.7)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth==2025.5.7)\n",
      "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torchvision in /home/user/miniconda/lib/python3.12/site-packages (from unsloth==2025.5.7) (0.22.0)\n",
      "Collecting cut_cross_entropy (from unsloth-zoo==2025.5.8)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /home/user/miniconda/lib/python3.12/site-packages (from unsloth-zoo==2025.5.8) (11.2.1)\n",
      "Requirement already satisfied: regex in /home/user/miniconda/lib/python3.12/site-packages (from unsloth-zoo==2025.5.8) (2024.11.6)\n",
      "Requirement already satisfied: msgspec in /home/user/miniconda/lib/python3.12/site-packages (from unsloth-zoo==2025.5.8) (0.19.0)\n",
      "Requirement already satisfied: filelock in /home/user/miniconda/lib/python3.12/site-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.12/site-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/user/miniconda/lib/python3.12/site-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/user/miniconda/lib/python3.12/site-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/user/miniconda/lib/python3.12/site-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (0.5.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/miniconda/lib/python3.12/site-packages (from huggingface_hub->unsloth==2025.5.7) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/user/miniconda/lib/python3.12/site-packages (from huggingface_hub->unsloth==2025.5.7) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/user/miniconda/lib/python3.12/site-packages (from torch>=2.4.0->unsloth==2025.5.7) (1.11.1.6)\n",
      "Requirement already satisfied: rich in /home/user/miniconda/lib/python3.12/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (13.9.4)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/miniconda/lib/python3.12/site-packages (from diffusers->unsloth==2025.5.7) (8.6.1)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth==2025.5.7)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth==2025.5.7)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth==2025.5.7)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->unsloth==2025.5.7)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/user/miniconda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (3.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.12/site-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.12/site-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.12/site-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.12/site-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/user/miniconda/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/user/miniconda/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (2.19.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/miniconda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth==2025.5.7) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/user/miniconda/lib/python3.12/site-packages (from importlib-metadata->diffusers->unsloth==2025.5.7) (3.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/miniconda/lib/python3.12/site-packages (from jinja2->torch>=2.4.0->unsloth==2025.5.7) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniconda/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth==2025.5.7) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/user/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/user/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/user/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (1.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/user/miniconda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniconda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth==2025.5.7) (1.17.0)\n",
      "Downloading unsloth-2025.5.7-py3-none-any.whl (265 kB)\n",
      "Downloading unsloth_zoo-2025.5.8-py3-none-any.whl (146 kB)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m182.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m265.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.22-py3-none-any.whl (125 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m265.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m303.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-extensions, shtab, pyarrow, protobuf, hf_transfer, fsspec, docstring-parser, dill, typeguard, pandas, multiprocess, tyro, diffusers, datasets, cut_cross_entropy, bitsandbytes, accelerate, peft, unsloth-zoo, unsloth\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.33.1 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.7.0 bitsandbytes-0.46.0 cut_cross_entropy-25.1.1 datasets-3.6.0 diffusers-0.33.1 dill-0.3.8 docstring-parser-0.16 fsspec-2025.3.0 hf_transfer-0.1.9 multiprocess-0.70.16 pandas-2.2.3 peft-0.15.2 protobuf-3.20.3 pyarrow-20.0.0 pytz-2025.2 shtab-1.7.2 typeguard-4.4.2 typing-extensions-4.14.0 tyro-0.9.22 tzdata-2025.2 unsloth-2025.5.7 unsloth-zoo-2025.5.8 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install unsloth==2025.5.7 unsloth-zoo==2025.5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873d190-30ba-4536-9ddb-7c1030793b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab3900-3cf6-43b6-aff5-7107eb674a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089fc358-3668-430c-846c-14fa0a15c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /home/user/miniconda/lib/python3.12/site-packages (11.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd92300-a4a5-4e23-8197-c36cf4c1bd31",
   "metadata": {},
   "source": [
    "# GEMMMA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecfb1572-8b5a-4f4f-a0a7-1dcac1c32e31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.9.0.1.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 1024\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6b3eb-c22c-4067-9ebd-3b325df688a4",
   "metadata": {},
   "source": [
    "### We now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad8951f0-783b-494e-8755-5fcbc93d87e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 32, #8          # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 32, #8  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe952e-b68f-4043-8600-bb801650c769",
   "metadata": {},
   "source": [
    "# Preparacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "244a2def-9b74-456c-9efb-69fa2918a248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'nombre', 'pais', 'tema', 'trama', 'pregunta', 'respuesta'],\n",
       "    num_rows: 2283\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"somosnlp-hackathon-2025/ibero-tales-es\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8079d73b-40f0-4b81-97e0-1ed15fe0dadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 2283\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seleccionar y renombrar las columnas\n",
    "dataset = dataset.select_columns(['pregunta', 'respuesta']).rename_columns({\n",
    "    'pregunta': 'question',\n",
    "    'respuesta': 'answer'\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5feade5e-5de8-4c54-8d33-6fa4e8ac7cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Crea una historia basada en La Difunta Correa, con la trama 'una mujer vaga por las tierras argentinas, ofreciendo consuelo y guía a quienes lo necesitan' y el tema 'libertad y sanación'. ¿Cómo encuentran consuelo aquellos que la encuentran?\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52c32956-5dfe-4e4d-ad97-c67e5d9b9dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La Difunta Correa, liberada de su trágica muerte, deambula por las vastas planicies de Argentina. Con su espíritu libre y bondadoso, se convierte en un faro de esperanza para aquellos que se aventuran en las solitarias carreteras.\\n\\nUna noche, una joven pareja se pierde en su viaje hacia Mendoza. La desesperación los invade, pero entonces aparece La Difunta Correa, con su figura etérea y su dulce sonrisa. Les ofrece agua fresca de un cántaro y les indica el camino de regreso a la seguridad. La pareja, agradecida, escucha sus sabios consejos y encuentra consuelo en su presencia tranquilizadora.\\n\\nOtros viajeros, abatidos por la tristeza y la pérdida, también encuentran consuelo en sus brazos maternales. La Difunta Correa les brinda palabras de aliento y les ayuda a sanar sus corazones rotos. Con cada alma a la que ayuda, su espíritu se fortalece y su leyenda crece, convirtiéndola en un símbolo de libertad y esperanza para todos los argentinos.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8e522-aad2-4b01-8d69-41b5b62cfefd",
   "metadata": {},
   "source": [
    "We now create a system prompt which can be customized. We add 4 extra symbols for working out or thinking / reasoning sections and a final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ace8d19-2ed2-454a-9874-968683836b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resuelve el siguiente problema.  \\nPrimero, piensa en voz alta qué debes hacer, paso por paso y de forma resumida, entre <think> y </think>.  \\nLuego, da la respuesta final entre <SOLUTION> y </SOLUTION>.  \\nNo escribas nada fuera de ese formato.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define las etiquetas para separar razonamiento y solución\n",
    "reasoning_start = \"<think>\"\n",
    "reasoning_end = \"</think>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"Resuelve el siguiente problema.  \n",
    "Primero, piensa en voz alta qué debes hacer, paso por paso y de forma resumida, entre {reasoning_start} y {reasoning_end}.  \n",
    "Luego, da la respuesta final entre {solution_start} y {solution_end}.  \n",
    "No escribas nada fuera de ese formato.\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb1da2-8759-4b8b-823a-143eedc94eb8",
   "metadata": {},
   "source": [
    "Let's map the dataset! and see the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d0955f3-de68-4009-9e77-c27a021d6a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e00c9bee22438da0f39a9da9a7eea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Crea una historia basada en La Difunta Correa, con la trama 'una mujer vaga por las tierras argentinas, ofreciendo consuelo y guía a quienes lo necesitan' y el tema 'libertad y sanación'. ¿Cómo encuentran consuelo aquellos que la encuentran?\",\n",
       " 'answer': 'La Difunta Correa, liberada de su trágica muerte, deambula por las vastas planicies de Argentina. Con su espíritu libre y bondadoso, se convierte en un faro de esperanza para aquellos que se aventuran en las solitarias carreteras.\\n\\nUna noche, una joven pareja se pierde en su viaje hacia Mendoza. La desesperación los invade, pero entonces aparece La Difunta Correa, con su figura etérea y su dulce sonrisa. Les ofrece agua fresca de un cántaro y les indica el camino de regreso a la seguridad. La pareja, agradecida, escucha sus sabios consejos y encuentra consuelo en su presencia tranquilizadora.\\n\\nOtros viajeros, abatidos por la tristeza y la pérdida, también encuentran consuelo en sus brazos maternales. La Difunta Correa les brinda palabras de aliento y les ayuda a sanar sus corazones rotos. Con cada alma a la que ayuda, su espíritu se fortalece y su leyenda crece, convirtiéndola en un símbolo de libertad y esperanza para todos los argentinos.',\n",
       " 'prompt': [{'content': 'Resuelve el siguiente problema.  \\nPrimero, piensa en voz alta qué debes hacer, paso por paso y de forma resumida, entre <think> y </think>.  \\nLuego, da la respuesta final entre <SOLUTION> y </SOLUTION>.  \\nNo escribas nada fuera de ese formato.',\n",
       "   'role': 'system'},\n",
       "  {'content': \"Crea una historia basada en La Difunta Correa, con la trama 'una mujer vaga por las tierras argentinas, ofreciendo consuelo y guía a quienes lo necesitan' y el tema 'libertad y sanación'. ¿Cómo encuentran consuelo aquellos que la encuentran?\",\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": x[\"answer\"],\n",
    "})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64304180-f149-44ac-a022-8715927e021e",
   "metadata": {},
   "source": [
    "We create a regex format to match the reasoning sections and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1888c2a-c83a-4b0b-b470-4550221b9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Regex para detectar razonamiento y solución juntos\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfa6c5-8ed1-49a7-8c55-847c69829d4b",
   "metadata": {},
   "source": [
    "We verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "133eeedb-9049-4b7e-a762-e13db37f9021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 86), match='<think>Déjame pensar como....</think><SOLUTION>Es>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_format.search(\n",
    "    \"<think>Déjame pensar como....</think>\"\\\n",
    "    \"<SOLUTION>Esta historia cuenta como...</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819af22-255d-460b-91f6-2b4412c9bdd2",
   "metadata": {},
   "source": [
    "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4cca439c-9878-4226-b0d9-1840c32450cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que da 3 puntos si el formato completo es correcto\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f78484-e605-4291-8282-ff51d51c3c30",
   "metadata": {},
   "source": [
    "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3e58655-bade-4885-a185-a7e5d214be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que da puntos parciales si cada etiqueta aparece exactamente una vez\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Contamos cuántas palabras clave se veen. ¡Se penaliza si se veen demasiadas! \n",
    "        # Si se vee 1, ¡Adicionamos puntos!\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4954c-25ba-4d39-a071-27f7602e3cef",
   "metadata": {},
   "source": [
    "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "47c10c83-db05-43f1-af76-5cf0bbe10d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para comparar la solución generada con la respuesta esperada usando similitud simple\n",
    "def check_text_similarity(prompts, completions, answer, **kwargs):\n",
    "    scores = []\n",
    "    for completion, true_answer in zip(completions, answer):\n",
    "        response = completion[0][\"content\"]\n",
    "        match = match_format.search(response)\n",
    "        if not match:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        generated_answer = match.group(1).strip().lower()\n",
    "        true_answer = true_answer.strip().lower()\n",
    "\n",
    "        gen_words = set(generated_answer.split())\n",
    "        true_words = set(true_answer.split())\n",
    "        common = gen_words.intersection(true_words)\n",
    "\n",
    "        if len(true_words) == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "\n",
    "        similarity = len(common) / len(true_words)  # Valor entre 0 y 1\n",
    "        score = similarity * 3  # Máximo 3 puntos\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce30985-18dc-4d94-b81d-eb18141341fa",
   "metadata": {},
   "source": [
    "## Configuracion Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067120d2-372a-454c-904c-a73c3b3f8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5105ed6-6ae1-4930-87c1-d0beb33eea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda-qc\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648d410b-0733-413e-9009-92be474999f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iberotales-gemma-3-1b-it-es\n"
     ]
    }
   ],
   "source": [
    "wandb_project = \"iberotales-gemma-3-1b-it-es\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "ඞ = os.environ[\"WANDB_PROJECT\"]\n",
    "print(ඞ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb541157-0836-447b-9356-56736749627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  2 22:49:47 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L40S                    On  |   00000000:30:00.0 Off |                    0 |\n",
      "| N/A   35C    P0            100W /  350W |    4775MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             529      C   /home/user/miniconda/bin/python        4634MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c620fc5-9aaa-4cd3-8b38-fd7f9ad44f24",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f10fc60-60cd-4093-a220-96fe9fa6415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_length = 512\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\", #paged_adamw_8bit\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = 1024, # max_seq_length - max_prompt_length\n",
    "    num_train_epochs = 3, # Set to 1 for a full training run\n",
    "    max_steps = 50,\n",
    "    save_steps = 50,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"wandb\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529480c-705b-4d5d-b7bc-651a23f33487",
   "metadata": {},
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the reward column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4691c1e5-44cd-4f71-b506-a0ea5ca9f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,283 | Num Epochs = 1 | Total steps = 50\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 26,091,520/1,025,977,472 (2.54% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 2:39:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / match_format_exactly</th>\n",
       "      <th>rewards / match_format_approximately</th>\n",
       "      <th>rewards / check_text_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.408629</td>\n",
       "      <td>2.790121</td>\n",
       "      <td>633.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.221129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.709521</td>\n",
       "      <td>2.485871</td>\n",
       "      <td>663.437500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.209521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.860035</td>\n",
       "      <td>1.313564</td>\n",
       "      <td>687.000000</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.047535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.524498</td>\n",
       "      <td>2.676237</td>\n",
       "      <td>646.187500</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.211998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.415219</td>\n",
       "      <td>2.523774</td>\n",
       "      <td>717.875000</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.352719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.821678</td>\n",
       "      <td>2.271278</td>\n",
       "      <td>660.437500</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.196679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.494104</td>\n",
       "      <td>2.576724</td>\n",
       "      <td>659.375000</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.244104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.683754</td>\n",
       "      <td>2.571864</td>\n",
       "      <td>603.250000</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.246254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.697543</td>\n",
       "      <td>3.088143</td>\n",
       "      <td>783.000000</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.260043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.220656</td>\n",
       "      <td>2.901351</td>\n",
       "      <td>726.750000</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.408156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.084202</td>\n",
       "      <td>2.967059</td>\n",
       "      <td>600.375000</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.271702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.032260</td>\n",
       "      <td>1.530460</td>\n",
       "      <td>617.312500</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.407260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.959616</td>\n",
       "      <td>2.930581</td>\n",
       "      <td>574.312500</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.397116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.619885</td>\n",
       "      <td>2.930350</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.369885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.597613</td>\n",
       "      <td>2.983010</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.347614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.457774</td>\n",
       "      <td>2.048359</td>\n",
       "      <td>630.625000</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.999159</td>\n",
       "      <td>2.514373</td>\n",
       "      <td>652.500000</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.249159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.959939</td>\n",
       "      <td>3.396383</td>\n",
       "      <td>778.000000</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.021824</td>\n",
       "      <td>2.495702</td>\n",
       "      <td>565.125000</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.271824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.462934</td>\n",
       "      <td>2.961374</td>\n",
       "      <td>582.875000</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.525434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.222022</td>\n",
       "      <td>2.193609</td>\n",
       "      <td>615.375000</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.597022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.268305</td>\n",
       "      <td>2.174958</td>\n",
       "      <td>630.000000</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.455805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.705282</td>\n",
       "      <td>2.098987</td>\n",
       "      <td>711.875000</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.767782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.783705</td>\n",
       "      <td>2.982069</td>\n",
       "      <td>678.937500</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.533705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.740682</td>\n",
       "      <td>2.718983</td>\n",
       "      <td>558.500000</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.490682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.246560</td>\n",
       "      <td>2.912975</td>\n",
       "      <td>556.437500</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.434060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.661901</td>\n",
       "      <td>1.274525</td>\n",
       "      <td>531.187500</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.661901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.539509</td>\n",
       "      <td>3.656807</td>\n",
       "      <td>701.625000</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.477009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.429493</td>\n",
       "      <td>2.893030</td>\n",
       "      <td>646.187500</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.491993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.920332</td>\n",
       "      <td>1.324881</td>\n",
       "      <td>550.437500</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>0.670332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.010685</td>\n",
       "      <td>2.038073</td>\n",
       "      <td>554.687500</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.573185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.916282</td>\n",
       "      <td>2.120575</td>\n",
       "      <td>570.875000</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.541282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.753033</td>\n",
       "      <td>2.241150</td>\n",
       "      <td>626.687500</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.503033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.981274</td>\n",
       "      <td>2.704737</td>\n",
       "      <td>614.875000</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.543774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.828608</td>\n",
       "      <td>2.164798</td>\n",
       "      <td>531.250000</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.516108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.903203</td>\n",
       "      <td>1.431138</td>\n",
       "      <td>612.937500</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.715703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.143939</td>\n",
       "      <td>2.281710</td>\n",
       "      <td>601.250000</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.581439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.563154</td>\n",
       "      <td>2.683528</td>\n",
       "      <td>603.750000</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.500654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.539884</td>\n",
       "      <td>2.327926</td>\n",
       "      <td>572.062500</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.727384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.730264</td>\n",
       "      <td>2.389340</td>\n",
       "      <td>758.750000</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.730264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.476747</td>\n",
       "      <td>2.071070</td>\n",
       "      <td>610.937500</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.601747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.429804</td>\n",
       "      <td>1.404878</td>\n",
       "      <td>556.375000</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.617304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>4.061497</td>\n",
       "      <td>2.265776</td>\n",
       "      <td>548.187500</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.623997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.575101</td>\n",
       "      <td>1.382585</td>\n",
       "      <td>663.312500</td>\n",
       "      <td>0.004621</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.700102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.898557</td>\n",
       "      <td>1.581852</td>\n",
       "      <td>559.500000</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.773558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.383545</td>\n",
       "      <td>1.953217</td>\n",
       "      <td>612.500000</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0.696044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>5.180675</td>\n",
       "      <td>1.230206</td>\n",
       "      <td>574.812500</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>0.680675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.623836</td>\n",
       "      <td>2.391855</td>\n",
       "      <td>568.750000</td>\n",
       "      <td>0.004334</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>4.204919</td>\n",
       "      <td>1.927577</td>\n",
       "      <td>500.562500</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.517419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.247152</td>\n",
       "      <td>1.879943</td>\n",
       "      <td>574.312500</td>\n",
       "      <td>0.006272</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.559652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.00013613314047688618, metrics={'train_runtime': 9811.2194, 'train_samples_per_second': 0.082, 'train_steps_per_second': 0.005, 'total_flos': 0.0, 'train_loss': 0.00013613314047688618})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_text_similarity,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151077c-35e6-4149-923c-5c52cde6c1d1",
   "metadata": {},
   "source": [
    "# INFERENCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6eec413a-6519-4f12-8dc9-47a1783b1abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "Resuelve el siguiente problema.  \n",
      "Primero, piensa en voz alta qué debes hacer, paso por paso y de forma resumida, entre <think> y </think>.  \n",
      "Luego, da la respuesta final entre <SOLUTION> y </SOLUTION>.  \n",
      "No escribas nada fuera de ese formato.\n",
      "\n",
      "Crea una historia corta basada en Wiracocha, con la trama 'Héroe<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<think>\n",
      "Okay, el problema es crear una historia corta basada en Wiracocha, enfocándose en el personaje del \"Héroe\".  Esto implica un equilibrio entre la descripción de Wiracocha y la construcción de un personaje que podamos conectar con.  Necesito una historia con un conflicto y una resolución que sea coherente con su filosofía.  Haré de la historia un relato breve, con un toque de misterio y un final que permita a Wiracocha reflexionar sobre su propio legado. No quiero una historia extensa, solo una narración concisa.\n",
      "</think>\n",
      "<SOLUTION>\n",
      "El río color esmeralda, Viru, serpenteaba entre las montañas, sus aguas silenciosas guardaban secretos ancestrales. Wiracocha, el Héroe, se había adentrado en el bosque como siempre, buscando la Concha de la Luna, una reliquia que, según la leyenda, podía curar cualquier enfermedad y asegurar la paz de las tribus.  No era un guerrero, ni un rey, sino un sanador, un recolector de conocimientos.  Su búsqueda lo llevó a un pueblo olvidado, consumido por la polución del río, donde la gente languidecía y la tierra se marchitaba.  Se encontró con la anciana Yma, la última guardiana de la sabiduría, que le reveló la verdad: la contaminación era una consecuencia de la ambición humana, un legado de la sociedad que había olvidado su conexión con la tierra.  Wiracocha, con su conocimiento de las plantas y los ciclos naturales, no intentó imponer soluciones rápidas. En cambio, guio a la gente a la creación de sistemas de filtración para el río, utilizando técnicas ancestrales de cultivo y la energía del bosque.  El río, revitalizado, volvió a ser la fuente de vida del pueblo. Wiracocha no encontró la Concha, pero la encontró en el alma de los habitantes, en su renovada conexión con la tierra.  De regreso, se marchó, sabiendo que su verdadero legado no era un objeto, sino la capacidad de enseñar y la sabiduría de un camino que no buscaba imponer, sino guiar.  El río, ahora limpio, reflejaba su propia sombra, un símbolo de esperanza y un recordatorio de la importancia del equilibrio. </SOLUTION><end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"Crea una historia corta basada en Wiracocha, con la trama 'Héroe'\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 512, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a75e02-1529-411a-9920-eb76245bb5dc",
   "metadata": {},
   "source": [
    "# Guardado o carga modelos alineados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7eae3-ee59-4eae-aa95-e97b8093aab3",
   "metadata": {},
   "source": [
    "Para guardar el modelo final como adaptadores LoRA, puedes usar `push_to_hub` de Hugging Face para guardarlo en línea o `save_pretrained` para una copia local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2fe9b71b-1af7-4ac6-9384-633dd25f3fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7dcabc030546708a3c3fe6b588e79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08e12a-0bca-400d-ab69-be7048082926",
   "metadata": {},
   "source": [
    "## Guardar adaptador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25560e96-b831-47f8-8ee3-882d45e896d8",
   "metadata": {},
   "source": [
    "**[NOTA]** Esto SOLO guarda los adaptadores LoRA, no el modelo completo. Para guardarlo en 16 bits o en formato GGUF, desplázate hacia abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d0a956f6-c180-4168-934b-91bf9812d8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-3/tokenizer_config.json',\n",
       " 'gemma-3/special_tokens_map.json',\n",
       " 'gemma-3/tokenizer.model',\n",
       " 'gemma-3/added_tokens.json',\n",
       " 'gemma-3/tokenizer.json')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gemma-3\")   # guardar localmente\n",
    "tokenizer.save_pretrained(\"gemma-3\") # guardar localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5b1125f8-928c-4981-bf9a-9761864e1842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdc4e31dd00459ba506ba84c3c6f956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9c80284d0b49b492fd8b763a1a12b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/daqc/iberotales-gemma-3-1b-it-es\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7c5f630b214750af2f5e3030f2d1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/38.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(\"daqc/iberotales-gemma-3-1b-it-es-adapter\") # guardar Online\n",
    "tokenizer.push_to_hub(\"daqc/iberotales-gemma-3-1b-it-es-adapter\") # guardar Online "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1541cda-5c6f-4026-a63e-86490f0bbbc4",
   "metadata": {},
   "source": [
    "## Guardando en float16 para VLLM\n",
    "Guuarda directamente en float16 para su despliegue. Se guarda en la carpeta gemma-3-finetune. Cámbialo de False a True para permitir que se ejecute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "777f3e5e-9b9d-4ecf-9f3f-df48b48d0e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/user/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Successfully copied all 1 files from cache to gemma-3-finetune.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 1/1 [00:09<00:00,  9.24s/it]\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6d67d-e615-4fbc-a181-271210555121",
   "metadata": {},
   "source": [
    "Si quieres subir o hacer push a tu cuenta de Hugging Face, cambia de False a True, agrega tu token de Hugging Face y la ubicación de subida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1dccea8b-0fc7-4e11-a7f7-bc4c5e26da03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48263845b0b449df9d30da7d7ef94848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/38.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/user/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Successfully copied all 1 files from cache to daqc/iberotales-gemma-3-1b-it-es-finetune.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc6b7abcce14607b30381eecbf04324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 1/1 [00:21<00:00, 21.34s/it]\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\"daqc/iberotales-gemma-3-1b-it-es-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108104e-aec1-4857-9834-31487279d398",
   "metadata": {},
   "source": [
    "## Conversión a GGUF / llama.cpp\n",
    "\n",
    "Guarda en `GGUF` / `llama.cpp`. Puedes convertir a precisiones `Q8_0`, `F16` o `BF16`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e074021d-9dd4-4cec-b57a-9fe23db5dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3-finetune\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3ForCausalLM\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {1152, 262144}\n",
      "Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {1152}\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "..... Chat template truncated .....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766f2a8587f74fa0aa7838111bbc347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
      "Unsloth: Converted to gemma-3-finetune.Q8_0.gguf with size = 1.1G\n",
      "Unsloth: Successfully saved GGUF to:\n",
      "gemma-3-finetune.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3-finetune\",\n",
    "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c179f9-95c6-411a-b66a-9c1b3a308984",
   "metadata": {},
   "source": [
    "Likewise, if you want to instead push to GGUF to your Hugging Face account, set if False to if True and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d604f20b-3199-4329-86ee-939e657c9b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3-finetune\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3ForCausalLM\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {1152, 262144}\n",
      "Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {1152}\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "..... Chat template truncated .....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4220063050c24dda818e6f75bd13af32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
      "Unsloth: Converted to gemma-3-finetune.Q8_0.gguf with size = 1.1G\n",
      "Unsloth: Successfully saved GGUF to:\n",
      "gemma-3-finetune.Q8_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to upload GGUF\n",
    "    model.push_to_hub_gguf(\n",
    "        \"gemma-3-finetune\",\n",
    "        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "        repo_id = \"daqc/iberotales-gemma-3-1b-it-es-finetune-gguf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd26e6-6973-438b-84d4-4cd011a792e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
