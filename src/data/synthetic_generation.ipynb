{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Downloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
      "\u001b[K     |████████████████████████████████| 259 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 85.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/user/miniconda/lib/python3.9/site-packages (4.61.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from cohere) (2.32.3)\n",
      "Collecting fastavro<2.0.0,>=1.9.4\n",
      "  Downloading fastavro-1.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 94.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<1,>=0.15\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 78.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from cohere) (4.13.2)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /home/user/miniconda/lib/python3.9/site-packages (from cohere) (0.28.1)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2\n",
      "  Downloading pydantic_core-2.34.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 84.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx-sse==0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0\n",
      "  Downloading types_requests-2.32.0.20250515-py3-none-any.whl (20 kB)\n",
      "Collecting pydantic>=1.9.2\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "\u001b[K     |████████████████████████████████| 444 kB 77.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna in /home/user/miniconda/lib/python3.9/site-packages (from httpx>=0.21.2->cohere) (2.10)\n",
      "Requirement already satisfied: certifi in /home/user/miniconda/lib/python3.9/site-packages (from httpx>=0.21.2->cohere) (2021.5.30)\n",
      "Requirement already satisfied: anyio in /home/user/miniconda/lib/python3.9/site-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/user/miniconda/lib/python3.9/site-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/user/miniconda/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 75.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->cohere) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "\u001b[K     |████████████████████████████████| 512 kB 105.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.2 MB 69.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 104.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 105.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 108.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniconda/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 97.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.22.4\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.5 MB 49.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/user/miniconda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx>=0.21.2->cohere) (1.3.0)\n",
      "Installing collected packages: urllib3, hf-xet, fsspec, filelock, typing-inspection, pydantic-core, huggingface-hub, annotated-types, tzdata, types-requests, tokenizers, pytz, pydantic, numpy, httpx-sse, fastavro, pandas, cohere\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.6\n",
      "    Uninstalling urllib3-1.26.6:\n",
      "      Successfully uninstalled urllib3-1.26.6\n",
      "Successfully installed annotated-types-0.7.0 cohere-5.15.0 fastavro-1.11.1 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.2 httpx-sse-0.4.0 huggingface-hub-0.32.3 numpy-2.0.2 pandas-2.2.3 pydantic-2.11.5 pydantic-core-2.33.2 pytz-2025.2 tokenizers-0.21.1 types-requests-2.32.0.20250515 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Instalación de dependencias\n",
    "!pip install cohere pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Inicializar el cliente de Cohere\n",
    "co = cohere.Client('YOUR_API_KEY_HERE')  # Reemplazar con tu API key\n",
    "MODEL_NAME = 'command-r-plus' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar datos\n",
    "def load_json_data(data_dir: str):\n",
    "    \"\"\"Cargar todos los archivos JSON del directorio de datos.\"\"\"\n",
    "    all_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.extend(data)\n",
    "    return all_data\n",
    "\n",
    "# Cargar los datos - Ajustado a tu ruta específica\n",
    "data_dir = '../../data/raw'  # Esta es la carpeta donde están tus JSON\n",
    "stories = load_json_data(data_dir)\n",
    "print(f\"Se cargaron {len(stories)} historias\")\n",
    "\n",
    "# Mostrar un ejemplo de la primera historia\n",
    "if stories:\n",
    "    print(\"\\nEjemplo de la primera historia:\")\n",
    "    print(json.dumps(stories[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the most common themes in literature\n",
    "Now let's access this site: https://prowritingaid.com/themes-in-literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_themes = [\n",
    "    \"Abuso de poder\", \"Adulterio\", \"Adversidad\", \"Envejecimiento\", \"Alienación\", \"Ambiciones\",\n",
    "    \"Sueño americano\", \"Arrogancia\", \"Arte\", \"Autonomía\", \"Belleza\", \"Creencias\", \"Traición\",\n",
    "    \"Valentía\", \"Capitalismo\", \"Celebración\", \"Azar\", \"Cambio versus tradición\",\n",
    "    \"Caos y orden\", \"Carácter\", \"Infancia\", \"Ciclo de la vida\", \"Clase social\", \"Cambio climático\",\n",
    "    \"Colonialismo\", \"Mayoría de edad\", \"Sentido común\", \"Comunicación\", \"Compañerismo\",\n",
    "    \"Conservación\", \"Conspiración\", \"Convención y rebelión\", \"Corrupción\", \"Coraje\",\n",
    "    \"Creación\", \"Crimen\", \"Oscuridad y luz\", \"Muerte\", \"Dedicación\", \"Democracia\",\n",
    "    \"Depresión\", \"Deseo\", \"Desesperación\", \"Destino\", \"Decepción\", \"Desilusión\",\n",
    "    \"Desplazamiento\", \"Sueños\", \"Economía\", \"Educación\", \"Empoderamiento\", \"Amor eterno\",\n",
    "    \"Fracaso\", \"Fe\", \"Fama\", \"Familia\", \"Destino\", \"Miedo\", \"Feminismo\", \"Amor prohibido\",\n",
    "    \"Perdón\", \"Libre albedrío\", \"Libertad\", \"Amistad\", \"Plenitud\", \"Futuro\",\n",
    "    \"Derechos LGBT\", \"Género\", \"Dios\", \"Bien contra mal\",\n",
    "    \"Gobierno\", \"Gratitud\", \"Codicia\", \"Crecimiento\", \"Culpa\", \"Felicidad\", \"Trabajo duro\",\n",
    "    \"Odio\", \"Salud\", \"Corazón roto\", \"Héroe\", \"Heroísmo\", \"Historia\", \"Honestidad\", \"Honor\",\n",
    "    \"Esperanza\", \"Humanidad\", \"Naturaleza humana\", \"Humildad\", \"Humor\", \"Hipocresía\", \"Identidad\",\n",
    "    \"Ideología\", \"Imaginación\", \"Inmortalidad\", \"Imperialismo\", \"Imposibilidad\", \"Individualidad\",\n",
    "    \"Desigualdad\", \"Injusticia\", \"Inocencia\", \"Inspiración\", \"Aislamiento\", \"Celos\", \"Alegría\",\n",
    "    \"Justicia\", \"Amabilidad\", \"Conocimiento\", \"Ley\", \"Legado\", \"Vida\", \"Soledad\", \"Pérdida\",\n",
    "    \"Amor\", \"Lealtad\", \"Locura\", \"Manipulación\", \"Materialismo\", \"Madurez\", \"Medicina\",\n",
    "    \"Recuerdos\", \"Misericordia\", \"Dinero\", \"Moralidad\", \"Maternidad\", \"Música\", \"Nacionalismo\", \"Naturaleza\",\n",
    "    \"Necesidad\", \"Negligencia\", \"Año nuevo\", \"Normalidad\", \"No rendirse\", \"Unidad\", \"Oportunidad\",\n",
    "    \"Opresión\", \"Optimismo\", \"Superación\", \"Pasión\", \"Paz\", \"Presión social\", \"Perfección\",\n",
    "    \"Perseverancia\", \"Desarrollo personal\", \"Política\", \"Pobreza\", \"Poder\", \"Oración\", \"Prejuicio\",\n",
    "    \"Orgullo\", \"Progreso\", \"Propaganda\", \"Propósito\", \"Realismo\", \"Realidad\", \"Rebelión\",\n",
    "    \"Renacimiento\", \"Redención\", \"Arrepentimiento\", \"Relaciones\", \"Religión\", \"Represión\", \"Resistencia\",\n",
    "    \"Venganza\", \"Revolución\", \"Sacrificio\", \"Tristeza\", \"Sátira\", \"Ciencia\", \"Autoconciencia\",\n",
    "    \"Autodisciplina\", \"Autosuficiencia\", \"Autopreservación\", \"Simplicidad\", \"Pecado\", \"Sociedad\",\n",
    "    \"Soledad\", \"Estoicismo\", \"Subjetividad\", \"Sufrimiento\", \"Vigilancia\", \"Supervivencia\",\n",
    "    \"Compasión\", \"Tecnología\", \"Tentación\", \"Tiempo\", \"Tolerancia\", \"Totalitarismo\", \"Tragedia\",\n",
    "    \"Viaje\", \"Confianza\", \"Verdad\", \"Amor incondicional\", \"Universo\", \"Amor no correspondido\", \"Desinterés\",\n",
    "    \"Valor\", \"Vanidad\", \"Vicios\", \"Violencia\", \"Virtud\", \"Guerra\", \"Desperdicio\", \"Riqueza\", \"Fuerza de voluntad\",\n",
    "    \"Ganar y perder\", \"Sabiduría\", \"Trabajo\", \"Luchas de la clase trabajadora\", \"Xenofobia\", \"Juventud\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_prompt(theme, original_story):\n",
    "    \"\"\"Crea un prompt para generar tramas basadas en un tema y una historia original.\"\"\"\n",
    "    prompt = f\"\"\"Eres un chatbot experto en crear tramas CORTAS de mitos y leyendas de España, Portugal y América Latina.\n",
    "    La respuesta debe ser directa, sin explicaciones, sin títulos y de frente la numeracion.\n",
    "    Prohibido superar 30 palabras por cada trama.\n",
    "    Por ejemplo: \n",
    "    <User>: Porfavor dame una trama de una historia basado en el siguiente contexto:\n",
    "    Tema: {theme}\n",
    "    Protagonista: {original_story['nombre']}\n",
    "    País: {original_story['pais']}\n",
    "    Descripción: {original_story['descripcion']}\n",
    "    Historia Original: {original_story['historia']}\n",
    "\n",
    "    <Assistant>:\n",
    "    1. Un curandero que descubre el poder de las hierbas sagradas en la selva amazónica.\n",
    "    2. Un chamán que puede comunicarse con los espíritus de los ancestros.\n",
    "\n",
    "    Ahora es tu turno. Basándote en el contexto proporcionado, crea solo 2 tramas diferentes para el protagonista, dame la numeración directamente.\n",
    "    Responde de manera creativa e innovadora y no repitas los ejemplos. ¡Cuanto más única y diferente sea tu respuesta, mejor ¡No pongas títulos a tus historias!\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_tramas(co, theme, original_story):\n",
    "    \"\"\"Genera una trama basada en un tema y una historia original.\"\"\"\n",
    "    prompt = create_topic_prompt(theme, original_story)\n",
    "    response = co.generate(\n",
    "        model=MODEL_NAME,  # Especificamos el modelo aquí\n",
    "        prompt=prompt,\n",
    "        max_tokens=128,\n",
    "        temperature=0.7,\n",
    "        k=0,\n",
    "        p=0.8,\n",
    "        frequency_penalty=0.1,\n",
    "        presence_penalty=0.1\n",
    "    )\n",
    "    return response.generations[0].text.strip()\n",
    "\n",
    "# Función para generar y guardar tramas\n",
    "def generate_and_save_tramas(co, stories, themes):\n",
    "    \"\"\"Genera tramas para cada historia usando 2 temas aleatorios diferentes y guarda incrementalmente.\"\"\"\n",
    "    # Intentar cargar tramas existentes si el archivo existe\n",
    "    try:\n",
    "        df_tramas = pd.read_csv('../../data/generated/tramas.csv')\n",
    "        existing_ids = set(df_tramas['id'].unique())\n",
    "    except FileNotFoundError:\n",
    "        df_tramas = pd.DataFrame(columns=['id', 'nombre', 'pais', 'Theme', 'Trama'])\n",
    "        existing_ids = set()\n",
    "    \n",
    "    for story in tqdm(stories, desc=\"Generando tramas\"):\n",
    "        # Saltar historias que ya fueron procesadas\n",
    "        if story['id'] in existing_ids:\n",
    "            continue\n",
    "            \n",
    "        # Seleccionar 2 temas aleatorios diferentes\n",
    "        selected_themes = random.sample(themes, 2)\n",
    "        \n",
    "        # Para cada tema seleccionado, generar una trama\n",
    "        for theme in selected_themes:\n",
    "            try:\n",
    "                # Generar trama\n",
    "                trama = generate_tramas(co, theme, story)\n",
    "                \n",
    "                # Crear nueva fila\n",
    "                new_row = {\n",
    "                    'id': story['id'],\n",
    "                    'nombre': story['nombre'],\n",
    "                    'pais': story['pais'],\n",
    "                    'Theme': theme,\n",
    "                    'Trama': trama\n",
    "                }\n",
    "                \n",
    "                # Agregar al DataFrame\n",
    "                df_tramas = pd.concat([df_tramas, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                \n",
    "                # Guardar después de cada trama generada\n",
    "                df_tramas.to_csv('../../data/generated/tramas.csv', index=False)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al generar trama para {story['id']} con tema {theme}: {str(e)}\")\n",
    "                # Guardar el progreso actual\n",
    "                df_tramas.to_csv('../../data/generated/tramas.csv', index=False)\n",
    "                continue\n",
    "    \n",
    "    return df_tramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando tramas: 100%|██████████| 243/243 [49:36<00:00, 12.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de tramas generadas:\n",
      "      id                                  nombre       pais     Theme  \\\n",
      "0  ar_01                       La Difunta Correa  Argentina  Libertad   \n",
      "1  ar_01                       La Difunta Correa  Argentina   Soledad   \n",
      "2  ar_02                                El Futre  Argentina  Política   \n",
      "3  ar_02                                El Futre  Argentina  Tristeza   \n",
      "4  ar_03  Leyenda del Cerro de los Siete Colores  Argentina    Música   \n",
      "\n",
      "                                               Trama  \n",
      "0  1. La Difunta Correa, liberada de su mortal pr...  \n",
      "1  1. La Difunta Correa, en su soledad, se convie...  \n",
      "2  1. El Futre, en busca de venganza, forma una a...  \n",
      "3  1. El Futre, un espíritu sin descanso, encuent...  \n",
      "4  1. La Leyenda del Cerro de los Siete Colores c...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar la generación de tramas\n",
    "df_tramas = generate_and_save_tramas(co, stories, story_themes)\n",
    "print(\"\\nEjemplo de tramas generadas:\")\n",
    "print(df_tramas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tramas(trama_text):\n",
    "    \"\"\"Separa el texto de tramas en una lista de tramas individuales.\"\"\"\n",
    "    # Dividir por números seguidos de punto (1., 2., 3., etc.)\n",
    "    tramas = [t.strip() for t in trama_text.split('\\n') if t.strip()]\n",
    "    # Limpiar la numeración al inicio de cada trama\n",
    "    tramas = [t.split('.', 1)[1].strip() if '.' in t else t for t in tramas]\n",
    "    return tramas\n",
    "\n",
    "def create_combined_prompt(theme, trama, original_story):\n",
    "    \"\"\"Crea un prompt combinado para generar la instrucción y la historia.\"\"\"\n",
    "    prompt = f\"\"\"Eres un chatbot experto en generar prompts e historias inspiradas en mitos y leyendas iberoamericanas basados solo en la trama.\n",
    "    La respuesta debe ser directa, sin explicaciones, sin títulos y de frente.\n",
    "    Prohibido superar 3 parrafos por cada historia.\n",
    "    Tema: {theme}\n",
    "    Trama: {trama}\n",
    "    Pais: ({original_story['pais']})\n",
    "    Protagonista: {original_story['nombre']}\n",
    "    Descripción: {original_story['descripcion']}\n",
    "    Historia original: {original_story['historia']}\n",
    "\n",
    "    Primero, genera una instrucción-pregunta que será usada para crear una historia. Sigue este formato:\n",
    "    \"Crea una historia basada en [protagonista], con la trama '[trama]' y el tema '[tema]'. [Pregunta o enunciado específica sobre la historia]\"\n",
    "\n",
    "    Ejemplos de instrucciones:\n",
    "    - \"Crea una historia basada en la leyenda de El Cadejo, con la trama 'un joven enfrenta sus miedos durante un ritual nocturno' y el tema 'valentía frente a lo desconocido'. ¿Cómo cambia su vida luego de esa noche?\"\n",
    "    - \"A partir de la leyenda de La Llorona, crea una historia donde una niña escucha voces en el río y descubre un antiguo secreto familiar. El tema es 'culpa y redención'.\"\n",
    "\n",
    "    Luego, escribe la historia que responda a esa instrucción. La historia debe:\n",
    "    1. Mantener la esencia cultural y mágica de la historia original\n",
    "    2. Ser apropiada para niños\n",
    "    3. Tener un inicio, desarrollo y final claros\n",
    "    4. Preservar elementos clave de la historia original\n",
    "    5. No superar 3 párrafos, los parrafos son cortos.\n",
    "\n",
    "    Responde en este formato exacto:\n",
    "    question: [tu instrucción generada para generar la historia]\n",
    "    answer: [tu historia generada respondiendo a la instrucción]\n",
    "\n",
    "    Ahora es tu turno. Basándote en el contexto proporcionado, crea solo 1 historia sobre la trama para el protagonista.\n",
    "    Responde de manera creativa e innovadora y no repitas los ejemplos. ¡Cuanto más única y diferente sea tu respuesta, mejor ¡No pongas títulos a tus historias!\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def generate_and_save_stories(co, df_tramas, trama_files=None):\n",
    "    \"\"\"Genera historias basadas en las tramas y guarda incrementalmente.\"\"\"\n",
    "    # Intentar cargar historias existentes si el archivo existe\n",
    "    try:\n",
    "        df_stories = pd.read_csv('../../data/generated/historias.csv')\n",
    "        # Crear un conjunto de tuplas (id, trama) para verificar duplicados\n",
    "        existing_combinations = set(zip(df_stories['id'], df_stories['Trama']))\n",
    "        print(f\"Historias existentes: {len(df_stories)}\")\n",
    "    except FileNotFoundError:\n",
    "        df_stories = pd.DataFrame(columns=['id', 'nombre', 'pais', 'Theme', 'Trama', 'question', 'answer'])\n",
    "        existing_combinations = set()\n",
    "        print(\"No hay historias existentes\")\n",
    "    \n",
    "    # Si se proporcionan archivos de tramas, cargarlos\n",
    "    if trama_files:\n",
    "        import glob\n",
    "        import os\n",
    "        \n",
    "        # Buscar todos los archivos que coincidan con el patrón\n",
    "        trama_files = glob.glob(os.path.join('../../data/generated/', 'tramas*.csv'))\n",
    "        \n",
    "        if not trama_files:\n",
    "            raise FileNotFoundError(\"No se encontraron archivos de tramas\")\n",
    "        \n",
    "        # Cargar y combinar todos los DataFrames\n",
    "        dfs = []\n",
    "        total_tramas = 0\n",
    "        for file in trama_files:\n",
    "            print(f\"Cargando archivo: {file}\")\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "            # Contar el número total de tramas en cada archivo\n",
    "            for _, row in df.iterrows():\n",
    "                tramas = split_tramas(row['Trama'])\n",
    "                total_tramas += len(tramas)\n",
    "        \n",
    "        # Combinar todos los DataFrames\n",
    "        df_tramas = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"\\nTotal de tramas a procesar: {total_tramas}\")\n",
    "    \n",
    "    historias_generadas = 0\n",
    "    for _, row in tqdm(df_tramas.iterrows(), desc=\"Generando historias\"):\n",
    "        # Separar las tramas individuales\n",
    "        tramas = split_tramas(row['Trama'])\n",
    "        \n",
    "        # Procesar cada trama individualmente\n",
    "        for trama in tramas:\n",
    "            # Verificar si esta combinación específica de ID y trama ya existe\n",
    "            if (row['id'], trama) in existing_combinations:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Generar historia\n",
    "                prompt = create_combined_prompt(row['Theme'], trama, {\n",
    "                    'nombre': row['nombre'],\n",
    "                    'pais': row['pais'],\n",
    "                    'descripcion': row.get('descripcion', ''),\n",
    "                    'historia': row.get('historia', '')\n",
    "                })\n",
    "                \n",
    "                response = co.generate(\n",
    "                    model=MODEL_NAME,\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    k=0,\n",
    "                    p=0.8,\n",
    "                    frequency_penalty=0.1,\n",
    "                    presence_penalty=0.1\n",
    "                )\n",
    "                \n",
    "                # Separar la respuesta en question y answer\n",
    "                response_text = response.generations[0].text.strip()\n",
    "                try:\n",
    "                    question = response_text.split('answer:')[0].replace('question:', '').strip()\n",
    "                    answer = response_text.split('answer:')[1].strip()\n",
    "                except:\n",
    "                    print(f\"Error al procesar la respuesta para {row['id']} con trama: {trama[:50]}...\")\n",
    "                    continue\n",
    "                \n",
    "                # Crear nueva fila\n",
    "                new_row = {\n",
    "                    'id': row['id'],\n",
    "                    'nombre': row['nombre'],\n",
    "                    'pais': row['pais'],\n",
    "                    'Theme': row['Theme'],\n",
    "                    'Trama': trama,  # Ahora guardamos la trama individual\n",
    "                    'question': question,\n",
    "                    'answer': answer\n",
    "                }\n",
    "                \n",
    "                # Agregar al DataFrame\n",
    "                df_stories = pd.concat([df_stories, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                \n",
    "                # Guardar después de cada historia generada\n",
    "                df_stories.to_csv('../../data/generated/historias.csv', index=False)\n",
    "                \n",
    "                # Incrementar contador de historias generadas\n",
    "                historias_generadas += 1\n",
    "                if historias_generadas % 10 == 0:  # Mostrar progreso cada 10 historias\n",
    "                    print(f\"\\nHistorias generadas hasta ahora: {historias_generadas}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al generar historia para {row['id']} con trama: {trama[:50]}...\")\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                # Guardar el progreso actual\n",
    "                df_stories.to_csv('../../data/generated/historias.csv', index=False)\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nProceso completado. Total de historias generadas: {historias_generadas}\")\n",
    "    return df_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay historias existentes\n",
      "Cargando archivo: ../../data/generated/tramas.csv\n",
      "Cargando archivo: ../../data/generated/tramas2.csv\n",
      "\n",
      "Total de tramas a procesar: 2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 5it [03:19, 38.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 10it [05:35, 30.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 15it [07:41, 22.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 20it [10:21, 29.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 25it [12:31, 33.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 30it [15:12, 30.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 35it [17:14, 29.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 40it [19:52, 28.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 45it [22:04, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 50it [23:46, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 55it [25:40, 19.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 60it [27:12, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 65it [29:53, 32.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 70it [32:12, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 75it [34:03, 21.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 80it [36:37, 30.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 85it [39:39, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 90it [47:31, 61.71s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 95it [50:30, 47.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 100it [55:37, 56.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 104it [56:38, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para cl_05 con trama: Colo Colo, una joven mapuche, descubre que su pueb...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 105it [1:01:45, 109.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 110it [1:04:00, 45.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 115it [1:05:23, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 120it [1:08:52, 39.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 125it [1:11:30, 36.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 130it [1:12:44, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 135it [1:14:32, 19.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 140it [1:16:36, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 145it [1:22:04, 57.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 150it [1:24:38, 31.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 155it [1:28:38, 42.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 160it [1:31:15, 29.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 165it [1:35:14, 57.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 170it [1:38:44, 40.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 175it [1:41:07, 34.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 180it [1:43:35, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 185it [1:47:35, 43.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 190it [1:49:31, 25.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 195it [1:52:19, 41.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 200it [1:56:52, 58.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 205it [1:57:56, 20.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 210it [1:59:46, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 215it [2:01:49, 26.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 220it [2:04:12, 30.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 225it [2:05:58, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 230it [2:07:40, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 235it [2:10:04, 22.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 240it [2:13:19, 33.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 245it [2:15:45, 32.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 250it [2:17:26, 22.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 255it [2:20:33, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 260it [2:22:10, 21.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 265it [2:25:37, 40.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 270it [2:27:49, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 275it [2:29:28, 22.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 280it [2:32:05, 33.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 285it [2:33:59, 20.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 290it [2:36:41, 29.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 295it [2:38:07, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 300it [2:40:38, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 305it [2:42:20, 23.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 310it [2:44:02, 20.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 315it [2:47:05, 35.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 320it [2:48:11, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 325it [2:51:08, 39.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 330it [2:53:30, 32.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 335it [2:55:20, 22.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 340it [2:57:27, 26.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 345it [2:58:45, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 350it [3:00:22, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 355it [3:01:47, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 360it [3:03:16, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 365it [3:05:17, 23.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 370it [3:06:35, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 375it [3:08:00, 17.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 380it [3:10:26, 24.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 385it [3:11:50, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 390it [3:13:20, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 395it [3:14:43, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 400it [3:16:02, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 405it [3:17:24, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 410it [3:18:56, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 415it [3:19:55, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 420it [3:21:02, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 425it [3:22:50, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 430it [3:24:15, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 435it [3:25:59, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 440it [3:29:32, 31.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 445it [3:32:19, 29.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 450it [3:34:43, 31.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 455it [3:36:54, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 460it [3:38:16, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 465it [3:40:14, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 470it [3:42:24, 27.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 475it [3:44:21, 26.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 480it [3:47:37, 30.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 485it [3:49:13, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 490it [3:51:50, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 495it [3:54:47, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 497it [3:55:47, 33.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para py_06 con trama: Ao Ao, la temible criatura, es capturada por un ex...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 501it [4:02:20, 59.58s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 506it [4:03:38, 21.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 511it [4:05:21, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 516it [4:06:23, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 517it [4:06:35, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para py_16 con trama: Un curandero llamado Suinda descubre un antiguo ri...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 521it [4:13:12, 52.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 526it [4:15:15, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 531it [4:17:46, 32.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 536it [4:19:37, 25.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 541it [4:21:14, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 546it [4:24:24, 35.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 551it [4:28:09, 45.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 556it [4:30:46, 39.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 561it [4:34:09, 43.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 566it [4:37:31, 36.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 571it [4:39:36, 27.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 576it [4:42:10, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 581it [4:45:50, 37.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 586it [4:47:22, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 591it [4:50:36, 30.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 596it [4:51:53, 17.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 601it [4:54:10, 26.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 605it [4:56:14, 30.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para pt_15 con trama: Un joven se adentra en la noche de San Juan en bus...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 607it [5:01:36, 83.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 612it [5:03:41, 34.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 617it [5:05:42, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 622it [5:07:10, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 627it [5:09:41, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 632it [5:10:58, 16.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 637it [5:14:28, 35.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 642it [5:17:30, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 647it [5:20:45, 38.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 652it [5:24:50, 48.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 657it [5:29:24, 67.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 662it [5:32:01, 28.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 667it [5:35:24, 41.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 672it [5:37:29, 32.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 677it [5:42:31, 49.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 682it [5:45:37, 41.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 687it [5:51:14, 56.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 692it [5:53:22, 26.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 697it [5:56:10, 32.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 699it [5:56:46, 25.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para pr_04 con trama: Guataubá, mensajero divino, es testigo de la injus...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 701it [6:04:25, 122.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para pr_05 con trama: La deidad, conmovida por la difícil situación de s...\n",
      "Error: status_code: 502, body: \n",
      "<html><head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<title>502 Server Error</title>\n",
      "</head>\n",
      "<body text=#000000 bgcolor=#ffffff>\n",
      "<h1>Error: Server Error</h1>\n",
      "<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n",
      "<h2></h2>\n",
      "</body></html>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 703it [6:05:02, 69.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 708it [6:07:22, 27.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 713it [6:10:07, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 718it [6:11:53, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 723it [6:15:35, 33.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 728it [6:17:53, 26.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 733it [6:19:45, 28.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 738it [6:22:52, 30.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 743it [6:25:06, 30.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 748it [6:26:16, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 753it [6:28:15, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 758it [6:32:00, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 763it [6:34:26, 33.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 768it [6:36:51, 28.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 773it [6:39:38, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 778it [6:41:08, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 783it [6:44:58, 45.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 788it [6:47:33, 36.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 793it [6:49:10, 22.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 798it [6:52:23, 37.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 803it [6:54:49, 34.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 808it [6:57:09, 24.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 813it [7:00:10, 39.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 818it [7:03:08, 32.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 823it [7:06:10, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 828it [7:07:46, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 833it [7:11:01, 41.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 838it [7:13:17, 25.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 843it [7:18:18, 36.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 848it [7:24:09, 47.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 853it [7:28:50, 57.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 858it [7:32:00, 36.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 863it [7:33:51, 20.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 868it [7:36:36, 29.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 873it [7:39:09, 30.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 878it [7:43:07, 49.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 883it [7:49:16, 73.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 888it [7:53:19, 57.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 893it [7:59:31, 62.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 897it [8:02:13, 42.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para pa_14 con trama: Un fotógrafo capta la imagen de La Silampa, revela...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 898it [8:07:19, 121.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 903it [8:11:45, 51.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 908it [8:12:51, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 913it [8:14:56, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 918it [8:19:11, 52.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 923it [8:21:32, 39.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 928it [8:24:10, 27.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 933it [8:28:44, 42.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 938it [8:30:57, 26.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 943it [8:32:31, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 948it [8:33:51, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 953it [8:36:48, 23.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 958it [8:39:23, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 963it [8:41:15, 20.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 968it [8:45:04, 43.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 973it [8:50:13, 79.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 978it [8:52:46, 36.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 983it [8:54:36, 24.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 988it [8:58:23, 48.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 993it [8:59:38, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 998it [9:01:09, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1003it [9:02:52, 23.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1008it [9:04:49, 24.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1013it [9:06:03, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1014it [9:06:40, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para mx_30 con trama: El Hombre del Costal se enfrenta a un grupo de niñ...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1019it [9:14:34, 52.77s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1024it [9:17:29, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1029it [9:19:25, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1034it [9:21:52, 28.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1039it [9:24:27, 25.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1044it [9:27:07, 29.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1049it [9:28:54, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1054it [9:31:42, 37.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1059it [9:33:51, 30.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1064it [9:39:21, 57.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1069it [9:42:27, 51.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1074it [9:48:37, 71.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1079it [9:52:27, 63.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1084it [9:55:59, 50.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1089it [9:58:37, 36.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1092it [10:05:30, 124.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al generar historia para pe_30 con trama: Chuichu, celoso de la adoración que reciben las de...\n",
      "Error: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1094it [10:06:28, 76.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1099it [10:08:26, 28.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1104it [10:10:25, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1109it [10:13:55, 42.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1114it [10:16:56, 48.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1119it [10:19:02, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1124it [10:22:44, 38.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1129it [10:25:44, 35.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1134it [10:28:32, 29.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1139it [10:30:42, 27.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1144it [10:32:48, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historias generadas hasta ahora: 2280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando historias: 1146it [10:34:03, 33.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceso completado. Total de historias generadas: 2283\n",
      "\n",
      "Ejemplo de historias generadas:\n",
      "      id             nombre       pais     Theme  \\\n",
      "0  ar_01  La Difunta Correa  Argentina  Libertad   \n",
      "1  ar_01  La Difunta Correa  Argentina  Libertad   \n",
      "2  ar_01  La Difunta Correa  Argentina   Soledad   \n",
      "3  ar_01  La Difunta Correa  Argentina   Soledad   \n",
      "4  ar_02           El Futre  Argentina  Política   \n",
      "\n",
      "                                               Trama  \\\n",
      "0  La Difunta Correa, liberada de su mortal prisi...   \n",
      "1  La Difunta Correa, en un acto de desafío contr...   \n",
      "2  La Difunta Correa, en su soledad, se convierte...   \n",
      "3  La Difunta Correa, a través de su conexión sob...   \n",
      "4  El Futre, en busca de venganza, forma una alia...   \n",
      "\n",
      "                                            question  \\\n",
      "0  Crea una historia basada en La Difunta Correa,...   \n",
      "1  Crea una historia basada en La Difunta Correa,...   \n",
      "2  Crea una historia basada en La Difunta Correa,...   \n",
      "3  Crea una historia basada en La Difunta Correa,...   \n",
      "4  Crea una historia basada en El Futre, con la t...   \n",
      "\n",
      "                                              answer  \n",
      "0  La Difunta Correa, liberada de su trágica muer...  \n",
      "1  En una pequeña aldea argentina, las noches era...  \n",
      "2  En las vastas extensiones del desierto argenti...  \n",
      "3  En las áridas tierras de Argentina, la leyenda...  \n",
      "4  El Futre, un espíritu poderoso de las montañas...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Ejecutar la generación de historias\n",
    "df_stories = generate_and_save_stories(co, None, trama_files=True)\n",
    "# Celda 3: Ver los resultados\n",
    "print(\"\\nEjemplo de historias generadas:\")\n",
    "print(df_stories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
